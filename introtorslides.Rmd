---
title: "Introduction to R"
subtitle: ""
author: "Rob Willans"
institute: "NICE"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

background-image: url(https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/R_logo.svg/620px-R_logo.svg.png)

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

???

Image credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:R_logo.svg)

---
# Introduction

* Data Scientist in the Data and Analytics team

* Tend to work in SQL, Python but mostly R

???

For those who don’t know me, my name’s Rob and I work in the Data and Analytics Team as a Data Scientist.
Being a Data Scientist means I know just enough to be dangerous in about 3 programming languages, SQL, Python and R. And the language that I know most about and use most about is R, which I’ll be talking about today.

---
# In the session today...

* R as a programming language

* R Vs Python

* R Packages

* The RStudio IDE

* R in the data analysis pipeline

* Learning Resources

* Examples

???

For today, rather than talking through specific bits of syntax or ‘how to write a function’ in R, I’m just going to run through a bit of a general overview about what R tends to be used for, how it differs from other languages, and a couple of example use case of things I’ve done in R @ NICE. Hope that’s what everyone was expecting, but if need be we can do sessions on basic syntax at a later date.

Will talk about R's origins and usage as a programming language, how it differs 

---
# R as a programming language

* Developed in NZ in the 90s

* Based on the 'S' language created at Bell Labs

* Primarily a functional rather than object-orientated language

???

R was created in New Zealand, in the University of Auckland in the 90s. It is however based on the language ‘S’, which is a language created at Bell Labs in the 70s. So in some respects, it is quite an old language, though popularised relatively recently.

R has come to prominence as a language for statistical programming. It’s tending to replace languages like MATLAB / STATA as a tool for analysis, though it can be used as a general programming language and basic web development through extensions like R Shiny, which is a framework for developing web dashboards, R Markdown, which can be used to generate HTML and other documents, and other packages.

Because R’s origins lie S, and S is quite an old language, R is more of a functional than an object orientated language. R does have several object-orientated systems, but a lot of these are either under-the-hood, or are used for advanced programming. For most users (myself included), they’d stick to a predominantly functional paradigm, so in that respect it’s a bit easier for people not familiar with object-orientated systems to learn. 

---
# R as a programming language(2)

* Used primarily as 'statistical programming' language
  - Though can be used for more general use cases
    - Web dashboards with R Shiny
    - Generate HTML with R Markdown
    - Other extensions to link to APIs, interlink with other languages
    
* Idiosyncrasies
  - Vectorised by default
    - Loops are therefore (by convention) rarely used
  - Can be memory intensive
  - 'Base R' functions (by convention) often superseded by 'tidyverse' library equivalents
  - Lots of pipes


???

R is predominantly used as a 'statistical programming' language, so mainly used for analysis, and statistical or machine learning tasks. It can be used for more general use cases, and over recent years there are extensions and frameworks that mean that it can be used for some web development or publishing tasks, interact with other languages and APIs, and be more of a general programming language.

There are few quirks to the language, for example most operations are vectorised by default, so you’ll almost never write a loop in the same way you would do with other general programming languages. R can be notoriously memory-intensive so can be slow in environments where there’s not a lot of memory. Some of the base R functions are practically eschewed by a very influential set of libraries called the ‘tidyverse’, so in practice you’re often better starting with the tidyverse then learning the basic functions, which feels a bit roundabout, but in many cases the tidyverse equivalent functions are faster, have easier syntax, and will be recognised by more people. Because R code is functional, you end up ‘piping’ a lot of steps by passing objects through lots of functions to get to your output. This is analogous to method chaining in object orientated languages, but is *probably* used more in R than other languages

---
# R Vs Python

- Use cases for R overlap most heavily with Python (for data science)

- Historically, R was more 'stats-y', Python more of a general use programming language

- Development of code libraries since make the distinction (for data science) more or less moot

???

Being a more general programming language, Python doesn’t have the native support for statistics that R does, but various extensions and python libraries have meant the distinction between the two is a bit moot or based on preference. Similarly, historically R didn’t have any web app frameworks, or a lot of integration possibilities with other languages, but people have developed libraries for R which have expanded it’s reach too. 

We’re now at a point where if someone develops a popular library in R for data work, someone will develop an equivalent library in Python, and vice versa. There are niche cases where you might prefer one or the other, but generally speaking for most data work (not general programming or web development) the languages are interchangeable. 

---
# R packages

- *Huge* array of user packages (code libraries) available on [Cran](https://cran.r-project.org/)

- Often for quite niche analysis needs

- Weirdly funny ones
  - beepr (plays notification sound when analysis script completes)
  - fortunes (generates fortune cookie messages from R help forums)

```{r echo=TRUE}
library(fortunes)

fortune(192)

```

???

R has a very active user group, which means that there is a really extensive set of code libraries continually under development, for a lot of different needs. This means that even those new to R can often find a package with wrapper functions or implementations of complex tasks that mean the user just needs to run a single function to accomplish what they need to do.

As much of the R user base comes to it from academic or analysis, there’s lots of packages for particular statistical analyses, econometrics and the like. If there’s a specialised function or field, like econometrics / epidemiology / evidence synthesis, chances are that someone has already developed an r package to conduct those sorts of specialist analysis.

These can sometimes extend to the frivoulous, including the beepr package that will give you a soundtrack to celebrate when your script runs successfully, and the fortunes package which generates funny fortune cookie style quotes from R help forums. There used to be one which made a call to the Uber API to get a taxi, but that's now defunct.

---
# RStudio IDE

- Most R users will use the [RStudio IDE](https://www.rstudio.com/)

- VS supports R, but not fantastically well<sup>1</sup>

.footnote[
[1] In my opinion, anyway....
]

???

You can use R with Visual Studio or another IDE, but most users will use the dedicated RStudio IDE. That works really well with R, as you’d expect, as that’s what it was designed to do primarily, and has the standard features you want from an IDE, nice GIT integration, etc. I personally don’t really like how Visual Studio does R, but then again I write everything in RStudio, so I would say that wouldn’t I. If you’re happy with Visual Studio you might want to give that a go first, but if you’re starting with no experience with R then I’d recommend trying RStudio as an IDE. You can write python really well in RStudio now as well, and it does have support for other languages. Because Rstudio are one of the few people in the R ecosystem with a chargeable product and consequent income stream, they're fairly dominant.


---
# R in the data analysis pipeline

  Data -> 'Carpentry' -> Analysis -> Outputs

- 'Big' data best manipulated in query language or database
- R (mostly) comes in at the analysis stage
- Data stored in 'dataframe' object
- Run your analysis (whatever that is)
  - Machine learning
  - Statistical Analysis
- Output
  - Visualisations 
  - Compiled document
  - Web dashboard
  - Flat file of results


???

So typically, you’re going to see R used by analysts or data scientists, rather than as a general programming language, though it can be used as such.
In an analysis pipeline you’d use SQL or similar query language for the heavy lifting work getting data out of a database, then ideally leaving R or Python to run your statistical analysis, exporting results and data visualisations. You might end up doing some data carpentry or manipulation in R, which is fine, as long as the data source can be loaded into memory on the workstation you're running R on.

R has really nice packages to connect to databases and pass query language into them to get results out. Dbplyr is a nice package for this. Alternatively, R can read data in a variety of flat file and proprietary formats, including csv, excel files, STATA files, (RIS is a bit tricky :) ) 

Once you have your data, it’ll be stored in a dataframe object, equivalent to a python pandas dataframe, or database table. This gives your data a bit of structure, names each column etc so you can run operations on particular columns and rows using simpler syntax. You have the 

Then there’s the inglorious but necessary task of filtering, cleaning, aggregating, or otherwise manipulating your data into the format you need it for visualisation or analysis. Typically that’s done using a package called dplyr, which offers a relatively intuiative syntax where each step in the analysis is a ‘verb’ function. It’s also possible to do things equivalent to relational joins in dplyr, though of course if you can do these in the database back end, it’s often better, as R will load everything into memory so for really large tables it can start to complain. 

After that, you can use R to do what it does best, which is analysis. Base R has a *lot* of statistical functions baked in, there are numerous extension packages, including all the standard machine learning algorithms, text analysis, regression packages for various model building, etc. etc. These can be used to develop models to explain or predict various phenomena, categorise data points into particular buckets according to their properties, generate summary statistics, forecast things into the future, etc.

After you’ve got results for that, you can use R to generate graphics using a package called ggplot2. This package is used by the BBC for their graphics. You can also use R to create word documents, pdfs or HTML pages using RMarkdown, create an online slide deck like this using a package called xaringen. And you can also use R Shiny to create a web app or dashboard as a front-end for the analysis. Or you can just run off a flat file of results, dependent on what you want to get out.


---
# Learning R

## Online Courses

- [Dataquest.io](dataquest.io) / Udemy / Pluralsight
- My own intro to R sessions run through python club
  - [https://rob-willans.shinyapps.io/01-intro_to_r]
  - [https://rob-willans.shinyapps.io/02-Intro_to_R/]

## Books

- [R for Data Science](https://r4ds.had.co.nz/)
- [Advanced R](https://adv-r.hadley.nz/)

### Networks

- [NHS-R Community](https://nhsrcommunity.com/)
 - [Conference 1st-10th November](https://nhsrcommunity.com/nhs-r-community-conference-2021/)

???

So if that’s piqued your interest
Perfectly possible to self-teach yourself R from free resources, largely just a google search away. There are some relatively cheap paid web courses out there as well that do a reasonable job of curating a syllabus and presenting it well, like Dataquest.io, linkedin learning, and I’m sure there’s good stuff on Udemy or pluralsight as well. The constraint you have is really just finding time to learn it.

I’ve done two little intro sessions through the python club, which you can access through the links below.

Hadley Wickham’s R for Data Science book is also very good, which you can find at this link. For those of you who have prior development experience, you might try jumping in to Hadley’s advanced R book (link below), which discusses some of the intricacies and details of R programming. 

Honorable mention for the NHS-R Community Network who have a well populated slack channel to ask for help in. They also run training webinars semi-regularly. Worth signing up to. The 2021 conference is also just coming up so you can sign up for that if interested. There's training workshops earlier in the week before presentations and talks on using R in the healthcare sector between the 8th-10th.

I'll keep these notes up in case you want to look at the links later

---
# Examples

- Preprint scraper

- Trialtracker


???

I'll just close with a bit of a show and tell of a couple of things I've used R for at NICE. So the two things that have seen the most use at NICE are the covid pre-print scraper, and the trialtracker dashboard. I’ll talk a little bit about each to demonstrate what’s possible with R

The pre-print scraper is a lovely little example of use of R to solve a small problem. Prior to this, Information Services had to manually page through the covid collection preprints on medrxiv and biorxiv, the search capabilities of the website weren’t great, and at the time of implementation we were all running round with a lot of demands on people’s time, getting the rapid covid guidelines underway. So to save people’s time manually searching for and downloading citations, I wrote a little script to make the process a little easier.
The script itself is about 100 lines of code that queries the medrxiv/biorxiv covid API every morning at 8am, returns the most recent 500 records in json format, (more than enough for one day), reformats it into a RIS file, then emails the file out to interested parties in IS via a mandrill service.
What that means is that IS can upload the file daily into a maintained EPPI database of preprints, and use the much better search capabilities in EPPI rather than having to use the website and spending hours clicking back and forth, downloading citations one at a time. 
I think this is a nice little example showing how R can be used for web scraping and general programming use. There’s no analysis done here (though there could be), and it’s a nice automation tool which can make use of APIs at either end as a link in the chain for a data pipeline.
The other major thing I’ve written in R that might be of interest is the trialtracker dashboard. Some of you have probably seen this before, but if not, this is a dashboard and email alert system which summarises information from various trial registries, all in one place. Rather than manually checking websites once a month to see if a particular trial has reported, here I’m querying registry APIs for trials that we’re interested in tracking. The script keeps a local copy of information in a SQLite database and then flagging either when changes are noted, or when a pubmed search indicates a publication. There are email alerts for that too, and the whole thing also has an R Shiny dashboard front end [ip restricted to within VMWare though]
Aside from that, I’ve done some text analysis using social media data, which R can do fairly well, and I’m currently using R (and Python) in a more traditional research project looking at investigating the prevalence of long covid and patterns of diagnosis across the uk with a primary care records provider. 
